@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}

@article{Zheng2023,
   author = {Li Zheng and Konstantinos Karapiperis and Siddhant Kumar and Dennis M. Kochmann},
   doi = {10.1038/s41467-023-42068-x},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   title = {Unifying the design space and optimizing linear and nonlinear truss metamaterials by generative modeling},
   volume = {14},
   year = {2023}
}


@article{Yamada2022,
   author = {Yutaka Yamada and Simone Genovese and Cal Depew and Ralph Cos and Hiroshi Kuwahara and Taiga Inoue},
   doi = {10.1016/B978-0-323-85159-6.50035-X},
   issn = {1570-7946},
   journal = {Computer Aided Chemical Engineering},
   keywords = {Multi-objective Optimization,Process Design,Process Simulation},
   month = {1},
   pages = {211-216},
   publisher = {Elsevier},
   title = {Reduce Environmental Impact and Carbon Footprint for Cost Competitive Process Plant Design: Integrating AVEVATM Process Simulation with modeFRONTIER®},
   volume = {49},
   year = {2022}
}

@article{Archbold2024,
   author = {Thomas A. Archbold and Ieva Kazlauskaite and Fehmi Cirak},
   doi = {10.1016/J.CMA.2024.117423},
   issn = {0045-7825},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Bayesian inference,Gaussian processes,Multi-query problems,Robust optimisation,Surrogate modelling,Variational Bayes},
   month = {12},
   pages = {117423},
   publisher = {North-Holland},
   title = {Variational Bayesian surrogate modelling with application to robust design optimisation},
   volume = {432},
   year = {2024}
}

@inproceedings{Tripp2020,
   author = {Austin Tripp and Erik Daxberger and José Miguel Hernández-Lobato},
   issn = {10495258},
   booktitle = {Advances in Neural Information Processing Systems},
   title = {Sample-efficient optimization in the latent space of deep generative models via weighted retraining},
   volume = {2020-December},
   year = {2020}
}

@article{Keogh2017,
   author = {Eamonn Keogh and Abdullah Mueen},
   doi = {10.1007/978-1-4899-7687-1_192},
   isbn = {978-1-4899-7687-1},
   journal = {Encyclopedia of Machine Learning and Data Mining},
   pages = {314-315},
   publisher = {Springer, Boston, MA},
   title = {Curse of Dimensionality},
   url = {https://link.springer.com/referenceworkentry/10.1007/978-1-4899-7687-1_192},
   year = {2017}
}

@article{Jabn2024,
   author = {Jorge Jabón and Sergio Corbera and Roberto Álvarez and Rafael Barea},
   doi = {10.1007/s00158-024-03771-5},
   issn = {16151488},
   issue = {3},
   journal = {Structural and Multidisciplinary Optimization},
   title = {Aerodynamic shape optimization using graph variational autoencoders and genetic algorithms},
   volume = {67},
   year = {2024}
}


@article{Lew2021,
   author = {Andrew J. Lew and Markus J. Buehler},
   doi = {10.1016/j.finmec.2021.100054},
   issn = {26663597},
   journal = {Forces in Mechanics},
   keywords = {3D printing,AI,Autoencoder,Deep learning,Design,Machine learning,Mechanics,Optimization,Structural design,Topology optimization},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {Encoding and exploring latent design space of optimal material structures via a VAE-LSTM model},
   volume = {5},
   year = {2021}
}

@article{Hobbs2021,
   author = {Mark Christopher Hobbs},
   doi = {10.17863/CAM.78179},
   keywords = {Fracture,Model validation,Numerical model,Peridynamics,Quasi-brittle materials,Shear failure,Size effect},
   title = {Three-dimensional peridynamic modelling of quasi-brittle structural elements},
   url = {https://www.repository.cam.ac.uk/handle/1810/330738},
   year = {2021}
}

@article{Roy2008,
   author = {Rajkumar Roy and Srichand Hinduja and Roberto Teti},
   doi = {10.1016/J.CIRP.2008.09.007},
   issn = {0007-8506},
   issue = {2},
   journal = {CIRP Annals},
   keywords = {Algorithm,Design,Optimisation},
   month = {1},
   pages = {697-715},
   publisher = {Elsevier},
   title = {Recent advances in engineering design optimisation: Challenges and future trends},
   volume = {57},
   year = {2008}
}

@article{Hare2013,
   author = {Warren Hare and Julie Nutini and Solomon Tesfamariam},
   doi = {10.1016/J.ADVENGSOFT.2013.03.001},
   issn = {0965-9978},
   journal = {Advances in Engineering Software},
   keywords = {Derivative-free optimization,Heuristic methods,Non-gradient methods,Optimization,Structural engineering,Swarm methods},
   month = {5},
   pages = {19-28},
   publisher = {Elsevier},
   title = {A survey of non-gradient optimization methods in structural engineering},
   volume = {59},
   year = {2013}
}

@article{Bribiesca2008,
   author = {Ernesto Bribiesca},
   doi = {10.1016/J.PATCOG.2007.06.029},
   issn = {0031-3203},
   issue = {2},
   journal = {Pattern Recognition},
   keywords = {Brain images,Contact perimeter,Contact surface area,Discrete compactness,Fragmented objects,Measure of compactness,Porous objects,Shape analysis,Shape classification},
   month = {2},
   pages = {543-554},
   publisher = {Pergamon},
   title = {An easy measure of compactness for 2D and 3D shapes},
   volume = {41},
   year = {2008}
}

@article{Braden1986,
   author = {Bart Braden},
   doi = {10.2307/2686282},
   issn = {07468342},
   issue = {4},
   journal = {The College Mathematics Journal},
   title = {The Surveyor's Area Formula},
   volume = {17},
   year = {1986}
}

@article{Shi2010,
   author = {L. Shi and K. Rasheed},
   doi = {10.1007/978-3-642-10701-6_1},
   isbn = {978-3-642-10701-6},
   issn = {1867-4542},
   pages = {3-28},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A Survey of Fitness Approximation Methods Applied in Evolutionary Algorithms},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-10701-6_1},
   year = {2010}
}

@article{Ren2025,
   author = {Chongle Ren and Zhenyu Meng},
   doi = {10.1016/J.ASOC.2025.112727},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Differential evolution,Expensive optimization problem,Hybrid techniques,Parallel and distributed techniques,Surrogate model},
   month = {2},
   pages = {112727},
   publisher = {Elsevier},
   title = {A survey on expensive optimization problems using differential evolution},
   volume = {170},
   year = {2025}
}

@article{Huang2022,
   author = {Qijing Huang and Charles Hong and John Wawrzynek and Mahesh Subedar and Yakun Sophia Shao},
   doi = {10.1109/ISPASS55109.2022.00041},
   isbn = {9781665459549},
   journal = {Proceedings - 2022 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2022},
   keywords = {accelerator,design space exploration,latent space,representation learning,vae,variational autoencoder},
   pages = {277-287},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Learning A Continuous and Reconstructible Latent Space for Hardware Accelerator Design},
   year = {2022}
}

@article{Pimentel2022,
   author = {Andy D. Pimentel},
   doi = {10.1007/978-981-15-6401-7_23-1},
   isbn = {978-981-15-6401-7},
   journal = {Handbook of Computer Architecture},
   pages = {1-31},
   publisher = {Springer, Singapore},
   title = {Methodologies for Design Space Exploration},
   url = {https://link.springer.com/referenceworkentry/10.1007/978-981-15-6401-7_23-1},
   year = {2022}
}

@incollection{CARDOSO2017255,
title = {Chapter 8 - Additional topics},
editor = {João M.P. Cardoso and José Gabriel F. Coutinho and Pedro C. Diniz},
booktitle = {Embedded Computing for High Performance},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {255-280},
year = {2017},
isbn = {978-0-12-804189-5},
doi = {https://doi.org/10.1016/B978-0-12-804189-5.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128041895000089},
author = {João M.P. Cardoso and José Gabriel F. Coutinho and Pedro C. Diniz},
keywords = {Design space exploration (DSE), Autotuning, Runtime adaptivity, Simulated annealing, Multiobjective optimization, Multicriteria optimization, Multiversioning},
}

@article{Nardi2018,
   author = {Luigi Nardi and David Koeplinger and Kunle Olukotun},
   doi = {10.1109/MASCOTS.2019.00045},
   isbn = {9781728149509},
   issn = {15267539},
   journal = {Proceedings - IEEE Computer Society's Annual International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunications Systems, MASCOTS},
   keywords = {Design space exploration,Hardware design,Machine learning driven optimization,Optimizing compilers,Pareto-optimal front,Performance modeling},
   month = {10},
   pages = {347-358},
   publisher = {IEEE Computer Society},
   title = {Practical Design Space Exploration},
   volume = {2019-October},
   url = {https://arxiv.org/abs/1810.05236v3},
   year = {2018}
}


@article{Panerati2016,
   author = {Jacopo Panerati and Donatella Sciuto and Giovanni Beltrame},
   doi = {10.1007/978-94-017-7358-4_7-1},
   isbn = {978-94-017-7358-4},
   journal = {Handbook of Hardware/Software Codesign},
   pages = {1-29},
   publisher = {Springer, Dordrecht},
   title = {Optimization Strategies in Design Space Exploration},
   url = {https://link.springer.com/referenceworkentry/10.1007/978-94-017-7358-4_7-1},
   year = {2016}
}
@Inbook{Sivanandam2008,
author="Sivanandam, S.N.
and Deepa, S.N.",
title="Genetic Algorithms",
bookTitle="Introduction to Genetic Algorithms",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="15--37",
isbn="978-3-540-73190-0",
doi="10.1007/978-3-540-73190-0_2",
url="https://doi.org/10.1007/978-3-540-73190-0_2"
}

@article{SrinivasN1994,
   abstract = {In trying to solve multiobjective optimization problems, many traditional methods scalarize the objective vector into a single objective. In those cases, the obtained solution is highly sensitive t...},
   author = {SrinivasN. and DebKalyanmoy},
   doi = {10.1162/EVCO.1994.2.3.221},
   issn = {1063-6560},
   issue = {3},
   journal = {Evolutionary Computation},
   keywords = {Multiobjective optimization,nondominated sorting,phenotypic sharing,ranking selection},
   month = {9},
   pages = {221-248},
   publisher = {MIT PressPUB1010Cambridge, MA, USA},
   title = {Muiltiobjective optimization using nondominated sorting in genetic algorithms},
   volume = {2},
   url = {https://dl.acm.org/doi/10.1162/evco.1994.2.3.221},
   year = {1994}
}


@article{Kim2021,
   author = {Yongtae Kim and Youngsoo Kim and Charles Yang and Kundo Park and Grace X. Gu and Seunghwa Ryu},
   doi = {10.1038/s41524-021-00609-2},
   issn = {2057-3960},
   issue = {1},
   journal = {npj Computational Materials 2021 7:1},
   keywords = {Composites,Computational methods},
   month = {9},
   pages = {1-7},
   publisher = {Nature Publishing Group},
   title = {Deep learning framework for material design space exploration using active transfer learning and data augmentation},
   volume = {7},
   url = {https://www.nature.com/articles/s41524-021-00609-2},
   year = {2021}
}

@article{Taj2023,
   author = {Zia Ud Din Taj and Ahmad Bilal and Muhammad Awais and Shuaib Salamat and Messam Abbas and Adnan Maqsood},
   doi = {10.1016/J.AST.2023.108114},
   issn = {1270-9638},
   journal = {Aerospace Science and Technology},
   keywords = {CFD,Gaussian process,Multi-objective genetic algorithm,Multidisciplinary optimization,RCS,Shooting and bouncing rays},
   month = {2},
   pages = {108114},
   publisher = {Elsevier Masson},
   title = {Design exploration and optimization of aerodynamics and radar cross section for a fighter aircraft},
   volume = {133},
   year = {2023}
}

@article{Gibbs2011,
   author = {M. S. Gibbs and H. R. Maier and G. C. Dandy},
   doi = {10.1080/0305215X.2010.491547},
   issn = {0305215X},
   issue = {4},
   journal = {Engineering Optimization},
   title = {Relationship between problem characteristics and the optimal number of genetic algorithm generations},
   volume = {43},
   year = {2011}
}

@article{Serani2024,
   author = {Andrea Serani and Matteo Diez},
   keywords = {Active subspace ·,Autoencoders,Dimensionality reduction ·,Parametric model embedding ·,Principal component analysis ·,Representation learning ·,Shape optimization ·,Space reduction ·},
   month = {5},
   title = {A Survey on Design-space Dimensionality Reduction Methods for Shape Optimization},
   url = {https://arxiv.org/abs/2405.13944v1},
   year = {2024}
}

@article {Brown:2018:2518-6582:1,
title = "Gradient-based guidance for controlling performance in early design exploration",
journal = "Proceedings of IASS Annual Symposia",
parent_itemid = "infobike://iass/piass",
publishercode ="iass",
year = "2018",
volume = "2018",
number = "2",
pages = "1-8",
itemtype = "ARTICLE",
issn = "2518-6582",
eissn = "2518-6582",
url = "https://www.ingentaconnect.com/content/iass/piass/2018/00002018/00000002/art00018",
keyword = "gradient, optimization, designer control, computational design, performance-based guidance",
author = "Brown, Nathan C and Mueller, Caitlin T",

}

@article{laurenceau2010comparison,
  title={Comparison of gradient-based and gradient-enhanced response-surface-based optimizers},
  author={Laurenceau, J and Meaux, M and Montagnac, M and Sagaut, P},
  journal={AIAA journal},
  volume={48},
  number={5},
  pages={981--994},
  year={2010}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages={177--186},
  year={2010},
  organization={Springer}
}

@article{Li2022,
   abstract = {Machine learning (ML) has been increasingly used to aid aerodynamic shape optimization (ASO), thanks to the availability of aerodynamic data and continued developments in deep learning. We review the applications of ML in ASO to date and provide a perspective on the state-of-the-art and future directions. We first introduce conventional ASO and current challenges. Next, we introduce ML fundamentals and detail ML algorithms that have been successful in ASO. Then, we review ML applications to ASO addressing three aspects: compact geometric design space, fast aerodynamic analysis, and efficient optimization architecture. In addition to providing a comprehensive summary of the research, we comment on the practicality and effectiveness of the developed methods. We show how cutting-edge ML approaches can benefit ASO and address challenging demands, such as interactive design optimization. Practical large-scale design optimizations remain a challenge because of the high cost of ML training. Further research on coupling ML model construction with prior experience and knowledge, such as physics-informed ML, is recommended to solve large-scale ASO problems.},
   author = {Jichao Li and Xiaosong Du and Joaquim R.R.A. Martins},
   doi = {10.1016/J.PAEROSCI.2022.100849},
   issn = {0376-0421},
   journal = {Progress in Aerospace Sciences},
   keywords = {Aerodynamic shape optimization,Airfoil design,Computational fluid dynamics,Machine Learning,Neural networks},
   month = {10},
   pages = {100849},
   publisher = {Pergamon},
   title = {Machine learning in aerodynamic shape optimization},
   volume = {134},
   year = {2022}
}

@article{Gori2024,
   author = {Marco Gori and Alessandro Betti and Stefano Melacci},
   doi = {10.1016/B978-0-32-389859-1.00009-X},
   isbn = {978-0-323-89859-1},
   journal = {Machine Learning},
   month = {1},
   pages = {53-111},
   publisher = {Morgan Kaufmann},
   title = {Learning principles},
   year = {2024}
}

@inproceedings{guo2016convolutional,
  title={Convolutional neural networks for steady flow approximation},
  author={Guo, Xiaoxiao and Li, Wei and Iorio, Francesco},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={481--490},
  year={2016}
}

@article{Diez2024,
   author = {Matteo Diez and Andrea Serani},
   doi = {10.1080/09377255.2023.2270309/ASSET/8C973E5F-DB24-4BED-8525-9F2C457A6E12/ASSETS/GRAPHIC/YSTR_A_2270309_F0012_OC.JPG},
   issn = {20567111},
   issue = {2},
   journal = {Ship Technology Research},
   keywords = {Karhunen-Loève expansion,Shape optimization,design-space dimensionality reduction,principal component analysis,proper orthogonal decomposition},
   pages = {141-152},
   publisher = {Taylor and Francis Ltd.},
   title = {Design-space dimensionality reduction in global optimization of functional surfaces: recent developments and way forward},
   volume = {71},
   url = {https://doi.org/10.1080/09377255.2023.2270309},
   year = {2024}
}

@article{Deshpande2024,
   author = {Saurabh Deshpande and Hussein Rappel and Mark Hobbs and Stéphane P. A. Bordas and Jakub Lengiewicz},
   doi = {10.1016/j.cma.2025.117790},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Autoencoders,Deep neural networks,Finite element method,Gaussian process,Surrogate modeling,Uncertainty quantification},
   month = {7},
   publisher = {Elsevier B.V.},
   title = {Gaussian process regression + deep neural network autoencoder for probabilistic surrogate modeling in nonlinear mechanics of solids},
   volume = {437},
   url = {http://arxiv.org/abs/2407.10732 http://dx.doi.org/10.1016/j.cma.2025.117790},
   year = {2024}
}

@misc{Jacobs1933,
   author = {Eatman N Jacobs and Ward Kenneth E and Robert M Pinkerton},
   title = {An interim report on the stability and control of tailless airplanes - NASA Technical Reports Server (NTRS)},
   url = {https://ntrs.nasa.gov/citations/19930091873},
   year = {1933}
}
@article{Donnelly2024,

   author = {James Donnelly and Alireza Daneshkhah and Soroush Abolfathi},
   doi = {10.1016/J.ENGAPPAI.2023.107536},
   issn = {0952-1976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Autoencoder,Climate forecast,Gaussian process,Multi-task learning,Uncertainty quantification},
   month = {2},
   pages = {107536},
   publisher = {Pergamon},
   title = {Forecasting global climate drivers using Gaussian processes and convolutional autoencoders},
   volume = {128},
   year = {2024}
}

@article{Sorzano2014,
   author = {C. O. S. Sorzano and J. Vargas and A. Pascual Montano},
   keywords = {Data Mining,Dimensionality Reduction,Machine Learning,Statistics},
   month = {3},
   title = {A survey of dimensionality reduction techniques},
   url = {https://arxiv.org/abs/1403.2877v1},
   year = {2014}
}

@article{Mendez2022,
   author = {Miguel A. Mendez},
   doi = {10.1088/1361-6501/acaffe},
   issue = {4},
   journal = {Measurement Science and Technology},
   keywords = {Dimensionality reduction,ISOMAPs,kernel PCA,locally linear embedding},
   month = {8},
   publisher = {Institute of Physics},
   title = {Linear and Nonlinear Dimensionality Reduction from Fluid Mechanics to Machine Learning},
   volume = {34},
   url = {http://arxiv.org/abs/2208.07746 http://dx.doi.org/10.1088/1361-6501/acaffe},
   year = {2022}
}

@article{Bengio2012,
   author = {Yoshua Bengio and Aaron Courville and Pascal Vincent},
   doi = {10.1109/TPAMI.2013.50},
   issn = {01628828},
   issue = {8},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
   month = {6},
   pages = {1798-1828},
   pmid = {23787338},
   title = {Representation Learning: A Review and New Perspectives},
   volume = {35},
   url = {https://arxiv.org/abs/1206.5538v3},
   year = {2012}
}

@article{DAgostino2018,
   author = {Danny D’Agostino and Andrea Serani and Emilio F. Campana and Matteo Diez},
   doi = {10.1007/978-3-319-72926-8_11},
   isbn = {978-3-319-72926-8},
   issn = {1611-3349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deep autoencoder,Hull-form design,Kernel methods,Nonlinear dimensionality reduction,Shape optimization},
   pages = {121-132},
   publisher = {Springer, Cham},
   title = {Nonlinear Methods for Design-Space Dimensionality Reduction in Shape Optimization},
   volume = {10710 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-72926-8_11},
   year = {2018}
}

@article{Kingma2013,
   abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
   author = {Diederik P. Kingma and Max Welling},
   doi = {10.61603/ceas.v2i1.33},
   journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
   month = {12},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Auto-Encoding Variational Bayes},
   url = {https://arxiv.org/abs/1312.6114v11},
   year = {2013}
}

@article{Danhaive2022,
   abstract = {Designers increasingly rely on parametric design studies to explore and improve structural concepts based on quantifiable metrics, generally either by generating design variations manually or using optimization methods. Unfortunately, both of these approaches have important shortcomings: effectively searching a large design space manually is infeasible, and design optimization overlooks qualitative aspects important in architectural and structural design. There is a need for methods that take advantage of computing intelligence to augment a designer's creativity while guiding—not forcing—their search for better-performing solutions. This research addresses this need by integrating conditional variational autoencoders in a performance-driven design exploration framework. First, a sampling algorithm generates a dataset of meaningful design options from an unwieldy design space. Second, a performance-conditioned variational autoencoder with a low-dimensional latent space is trained using the collected data. This latent space is intuitive to explore by designers even as it offers a diversity of high-performing design options.},
   author = {Renaud Danhaive and Caitlin T. Mueller},
   doi = {10.1016/J.AUTCON.2021.103664},
   issn = {09265805},
   journal = {Other repository},
   keywords = {Article,Deep generative modeling,Design space,Latent space,Latent variable,Variational autoencoder},
   month = {7},
   publisher = {Elsevier BV},
   title = {Design subspace learning: Structural design space exploration using performance-conditioned generative modeling},
   volume = {127},
   url = {https://dspace.mit.edu/handle/1721.1/145568},
   year = {2022}
}

@article{Kingma2019,
   abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
   author = {Diederik P. Kingma and Max Welling},
   doi = {10.1561/2200000056},
   issn = {19358245},
   issue = {4},
   journal = {Foundations and Trends in Machine Learning},
   month = {6},
   pages = {307-392},
   publisher = {Now Publishers Inc},
   title = {An Introduction to Variational Autoencoders},
   volume = {12},
   url = {https://arxiv.org/abs/1906.02691v3},
   year = {2019}
}

@article{Kingma2014,
   author = {Diederik P. Kingma and Jimmy Lei Ba},
   journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   month = {12},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Adam: A Method for Stochastic Optimization},
   url = {https://arxiv.org/abs/1412.6980v9},
   year = {2014}
}

@article{FredAgarap2018,
   author = {Abien M Fred Agarap},
   keywords = {CCS CONCEPTS • Computing methodologies → Supervised learning by clas-sification,KEYWORDS artificial intelligence,Neural networks,artificial neural networks,classification,con-volutional neural network,deep learning,deep neural networks,feed-forward neural network,machine learning,rectified linear units,softmax,supervised learning},
   month = {3},
   title = {Deep Learning using Rectified Linear Units (ReLU)},
   url = {https://arxiv.org/abs/1803.08375v2},
   year = {2018}
}

@article{Hassanat2019,
   author = {Ahmad Hassanat and Khalid Almohammadi and Esra'a Alkafaween and Eman Abunawas and Awni Hammouri and V. B.Surya Prasath},
   doi = {10.3390/INFO10120390},
   issn = {2078-2489},
   issue = {12},
   journal = {Information 2019, Vol. 10, Page 390},
   keywords = {crossover,genetic algorithms,mutation,parameter selection,ratios},
   month = {12},
   pages = {390},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Choosing Mutation and Crossover Ratios for Genetic Algorithms—A Review with a New Dynamic Approach},
   volume = {10},
   url = {https://www.mdpi.com/2078-2489/10/12/390/htm https://www.mdpi.com/2078-2489/10/12/390},
   year = {2019}
}

@article{Hussain2020,
   author = {Abid Hussain and Yousaf Shad Muhammad},
   doi = {10.1007/S40747-019-0102-7/FIGURES/5},
   issn = {21986053},
   issue = {1},
   journal = {Complex and Intelligent Systems},
   keywords = {Genetic algorithm,Selection operators,Selection pressure,Statistical analysis,Traveling salesman problem},
   month = {4},
   pages = {1-14},
   publisher = {Springer International Publishing},
   title = {Trade-off between exploration and exploitation with genetic algorithm using a novel selection operator},
   volume = {6},
   url = {https://link.springer.com/article/10.1007/s40747-019-0102-7},
   year = {2020}
}

@article{OShea2015,
   abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
   author = {Keiron O'Shea and Ryan Nash},
   doi = {10.22214/ijraset.2022.47789},
   issue = {12},
   journal = {International Journal for Research in Applied Science and Engineering Technology},
   month = {11},
   pages = {943-947},
   publisher = {International Journal for Research in Applied Science and Engineering Technology (IJRASET)},
   title = {An Introduction to Convolutional Neural Networks},
   volume = {10},
   url = {https://arxiv.org/abs/1511.08458v2},
   year = {2015}
}

@article{Kucherenko2015,
   abstract = {Three sampling methods are compared for efficiency on a number of test problems of various complexity for which analytic quadratures are available. The methods compared are Monte Carlo with pseudo-random numbers, Latin Hypercube Sampling, and Quasi Monte Carlo with sampling based on Sobol sequences. Generally results show superior performance of the Quasi Monte Carlo approach based on Sobol sequences in line with theoretical predictions. Latin Hypercube Sampling can be more efficient than both Monte Carlo method and Quasi Monte Carlo method but the latter inequality holds for a reduced set of function typology and at small number of sampled points. In conclusion Quasi Monte Carlo method would appear the safest bet when integrating functions of unknown typology.},
   author = {Sergei Kucherenko and Daniel Albrecht and Andrea Saltelli},
   keywords = {High Dimensional Integration,Latin Hypercube Sampling,Monte Carlo,Quasi Monte Carlo,Sobol' sequences},
   month = {5},
   title = {Exploring multi-dimensional spaces: a Comparison of Latin Hypercube and Quasi Monte Carlo Sampling Techniques},
   url = {https://arxiv.org/abs/1505.02350v1},
   year = {2015}
}

@article{Boschini2025,
   abstract = {Latin Hypercube Sampling (LHS) is a prominent tool in simulation design, with a variety of applications in high-dimensional and computationally expensive problems. LHS allows for various optimization strategies, most notably to ensure space-filling properties. However, LHS is a single-stage algorithm that requires a priori knowledge of the targeted sample size. In this work, we present “LHS in LHS,” a new expansion algorithm for LHS that enables the addition of new samples to an existing LHS-distributed set while (approximately) preserving its properties. In summary, the algorithm identifies regions of the parameter space that are far from the initial set, draws a new LHS within those regions, and then merges it with the original samples. As a by-product, we introduce a new metric, the LHS degree, which quantifies the deviation of a given design from an LHS distribution. Our public implementation is distributed via the Python package EXPANDLHS.},
   author = {Matteo Boschini and Davide Gerosa and Alessandro Crespi and Matteo Falcone},
   doi = {10.1016/J.SOFTX.2025.102294},
   issn = {2352-7110},
   journal = {SoftwareX},
   keywords = {Latin hypercube sampling,Simulation design,Space-filling},
   month = {9},
   pages = {102294},
   publisher = {Elsevier},
   title = {“LHS in LHS”: A new expansion strategy for Latin hypercube sampling in simulation design},
   volume = {31},
   year = {2025}
}

@article{Mirza2014,
   abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
   author = {Mehdi Mirza and Simon Osindero},
   month = {11},
   title = {Conditional Generative Adversarial Nets},
   url = {https://arxiv.org/abs/1411.1784v1},
   year = {2014}
}

@article{Oseni2021,
   abstract = {The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications and reviewed several cyber defences that would protect AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.},
   author = {Ayodeji Oseni and Nour Moustafa and Helge Janicke and Peng Liu and Zahir Tari and Athanasios Vasilakos},
   doi = {10.1145/1122445.1122456},
   issue = {111},
   journal = {J. ACM},
   keywords = {Adversarial Attacks,Adversarial Examples,Deep Learning,Machine Learning,Privacy,Security},
   month = {2},
   pages = {35},
   title = {Security and Privacy for Artificial Intelligence: Opportunities and Challenges},
   volume = {37},
   url = {https://arxiv.org/abs/2102.04661v1},
   year = {2021}
}

@article{Druc2022,
   abstract = {We explore the interpretability of 3D geometric deep learning models in the context of Computer-Aided Design (CAD). The field of parametric CAD can be limited by the difficulty of expressing high-level design concepts in terms of a few numeric parameters. In this paper, we use a deep learning architectures to encode high dimensional 3D shapes into a vectorized latent representation that can be used to describe arbitrary concepts. Specifically, we train a simple auto-encoder to parameterize a dataset of complex shapes. To understand the latent encoded space, we use the idea of Concept Activation Vectors (CAV) to reinterpret the latent space in terms of user-defined concepts. This allows modification of a reference design to exhibit more or fewer characteristics of a chosen concept or group of concepts. We also test the statistical significance of the identified concepts and determine the sensitivity of a physical quantity of interest across the dataset.},
   author = {Stefan Druc and Aditya Balu and Peter Wooldridge and Adarsh Krishnamurthy and Soumik Sarkar},
   doi = {10.1109/CVPRW56347.2022.00338},
   isbn = {9781665487399},
   issn = {21607516},
   journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
   month = {4},
   pages = {2992-2999},
   publisher = {IEEE Computer Society},
   title = {Concept Activation Vectors for Generating User-Defined 3D Shapes},
   volume = {2022-June},
   url = {https://arxiv.org/abs/2205.02102v1},
   year = {2022}
}

@article{Yousefzadeh2022,
   abstract = {Many applications affecting human lives rely on models that have come to be known under the umbrella of machine learning and artificial intelligence. These AI models are usually complicated mathematical functions that map from an input space to an output space. Stakeholders are interested to know the rationales behind models' decisions and functional behavior. We study this functional behavior in relation to the data used to create the models. On this topic, scholars have often assumed that models do not extrapolate, i.e., they learn from their training samples and process new input by interpolation. This assumption is questionable: we show that models extrapolate frequently; the extent of extrapolation varies and can be socially consequential. We demonstrate that extrapolation happens for a substantial portion of datasets more than one would consider reasonable. How can we trust models if we do not know whether they are extrapolating? Given a model trained to recommend clinical procedures for patients, can we trust the recommendation when the model considers a patient older or younger than all the samples in the training set? If the training set is mostly Whites, to what extent can we trust its recommendations about Black and Hispanic patients? Which dimension (race, gender, or age) does extrapolation happen? Even if a model is trained on people of all races, it still may extrapolate in significant ways related to race. The leading question is, to what extent can we trust AI models when they process inputs that fall outside their training set? This paper investigates several social applications of AI, showing how models extrapolate without notice. We also look at different sub-spaces of extrapolation for specific individuals subject to AI models and report how these extrapolations can be interpreted, not mathematically, but from a humanistic point of view.},
   author = {Roozbeh Yousefzadeh and Xuenan Cao},
   doi = {10.1145/nnnnnnn.nnnnnnn},
   keywords = {datasets,explainable AI,extrapolation,machine learning},
   month = {1},
   title = {To what extent should we trust AI models when they extrapolate?},
   url = {https://arxiv.org/abs/2201.11260v1},
   year = {2022}
}

@article{Karniadakis2021,
   abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems. The rapidly developing field of physics-informed learning integrates data and mathematical models seamlessly, enabling accurate inference of realistic and high-dimensional multiphysics problems. This Review discusses the methodology and provides diverse examples and an outlook for further developments.},
   author = {George Em Karniadakis and Ioannis G. Kevrekidis and Lu Lu and Paris Perdikaris and Sifan Wang and Liu Yang},
   doi = {10.1038/s42254-021-00314-5},
   issn = {2522-5820},
   issue = {6},
   journal = {Nature Reviews Physics 2021 3:6},
   keywords = {Applied mathematics,Computational science},
   month = {5},
   pages = {422-440},
   publisher = {Nature Publishing Group},
   title = {Physics-informed machine learning},
   volume = {3},
   url = {https://www.nature.com/articles/s42254-021-00314-5},
   year = {2021}
}




























